<!DOCTYPE html>
<html lang="en">

<head>
<title>ABconvergence</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/morris.js/0.5.1/morris.css">
<script src="//ajax.googleapis.com/ajax/libs/jquery/3.1.0/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/raphael/2.1.0/raphael-min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/morris.js/0.5.1/morris.min.js"></script>
<script type="text/javascript" src="static/js/plotResult.js"></script>

<style type="text/css">
    body {
	padding-left: 10%;
	padding-right: 10%;
        line-height: 120%;
        text-align:justify;
    }

    .error {
      color:red;
      font-weight: bold;
    }

    footer {
      /*background-color: #736E6D;*/
      background-color: #228B22;
      color: black;
      padding: 5px;
      margin: 15px 0px;
      text-align: center;
    }

    h3 {
      font-size:15pt;
    }

</style>
</head>

<body>

<h3>Real-world hypothesis testing: a dependably treacherous adventure</h3>

<div id="mainText">
  <div style="width:20%;float:right;background-color: #FFFFC2;border-radius: 4px; border: 2px solid;padding:5px;margin-left:30px;margin-bottom:10px;">
    <div style="text-align:center;">
    <a href="https://github.com/Ranlot"><img src="static/css/GitHub_Logo.png" alt="GitHub" height="40px" /></a>
    <a href="https://il.linkedin.com/in/laurent-bouÃ©-b7923853"><img src="static/css/linkedin.png" height="40px" alt="LinkedIn" /></a>
    </div>
    <!--
    <div style="display:inline-block">
    More in-depth <a href="https://github.com/Ranlot/ab-testing">discussion</a> along with source code and mathematical derivations.
    </div> -->
  </div>
  <div>
    <div> 

	    <!-- <h3>Real-world hypothesis testing: a dependably treacherous adventure</h3> -->

<p> From modern day web-scale marketing campaigns to their <a href="https://en.wikipedia.org/wiki/William_Sealy_Gosset">scientific origins</a> at the Guinness Brewery, <a href="https://en.wikipedia.org/wiki/A/B_testing">AB tests</a> continue to stand as a pillar upon which <i>"intelligent"</i> business decisions are made.  Assessing the conclusiveness of such tests inescapably brings forth the arduous concept of <i>"<a href="https://en.wikipedia.org/wiki/Statistical_significance">statistical significance</a>"</i>.  Indeed, it is now being more and more <a href="https://www.sciencenews.org/blog/context/p-value-ban-small-step-journal-giant-leap-science">publicly acknowledged</a> that hypothesis testing is the point where a cloud of perplexed turbulence often starts to develop...  In addition to its inherently perilous conceptual subtleties,  real-world hypothesis testing has to deal with the fact that AB tests are constantly subjected to numerous and somewhat uncontrollable changes (deployment of new features, seasonality, special case scenarios...). Correlations, perceived or real, between all these effects often endow actionable business decisions reached on the basis of "statistical significance" with an unfortunate sense of murkiness. </p>

<p>Perversely, the purpose of this note is to exemplify one essential (and not widely recognized) factor that contributes to the hazards of hypothesis testing: <b>slow convergence due to finite size-effects</b>.  Note that we will do so by placing ourselves under the most ideal situations where everything about the AB test is known in advance and nothing changes during the experiment as described in the first section below.  The calculator presented in the second section below provides an implementation of the ideas that helps get some quantitative insight... </p>

<h3>Description of the idealized AB test</h3>

<p> In order to see the importance of finite-size effects, let us consider a series of N repeated events where each individual event can only support 2 possible outcomes: "success" with some probability p* and "failure" with probability of 1-p*.  For concreteness, one may think of coin flip in which p* represents the probability of observing a specific side (p* &ne; 1/2 for biased coins).  Alternatively, business intelligence readers may consider p* as the probability to click on a web page.  Obviously, the statistical properties of such <a href="https://en.wikipedia.org/wiki/Bernoulli_trial">Bernoulli trials</a> are well known:  The process eventually converges to a Gaussian distribution of mean value p* and of standard deviation proportional to N<sup>-1/2</sup> (decreasing as the inverse square root of the number of events).
</p>

<p> Now that we know how our events are generated, let us further assume that (perhaps through previous experience) we are already aware of some "baseline success rate" p0 against which we would like to test the performance of our experiment.  The traditional technique consists in measuring our "empirical success rate" p and in feeding it along with p0 into a statistical significance test in order to extract a p-value. According to common practice, the statistical test is the so-called "<a href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing">one-proportion test</a>".  If the resulting p-value is smaller than 0.05, we can declare the test as conclusive meaning that the difference between p and p0 is indeed "statistically significant".  Here, it is crucial to realize that because of the finiteness of the number of events N, p is random variable which is only <b>on average</b> equal to the "true success rate" p*.  Unfortunately, one is rarely in a position to run multiple realizations of the experiment and must usually be satisfied with a single value p of the empirical success rate whose deviation away from p* depend on the sample size N.
</p>

<h3>The crux of the problem: finite-size effects & slow convergence</h3>

<p> One important consequence is that the p-value obtained by the single test described above cannot be taken at face value since it is itself a random variable with a complicated statistical distribution.  Therefore, an interesting question becomes: </p>

<div style="width:78%;margin-left:10%;background-color: #FFFFC2; border-color: #000000; border-radius: 4px; border: 1px solid; padding: 4px 8px;">
<p> Given a "baseline rate" p0, a "true success rate" p* &ge; p0, how many events N do we need until <b>our single test</b> would yield a significant conclusion (as it should) with probability of at least 0.95? The calculator below allows you to explore this question and discover a surprisingly slow convergence rate leading to rather high values of N...</p>
</div>

<p> Note that the tool below allows one to also investigate the opposite situation where p* &le; p0.  In this case, we may be interested to know how many events it takes such that the experiment would yield a significant result (clearly the wrong conclusion) with a probability of no more than 0.01.
</p>

<h3>Slow convergence exposed: see for yourself</h3>

<p><i> Guide: Valid probabilities can only take values between 0 and 1 exclusive. For example, you can start with p* = 0.51 and p0 = 0.5 in order to see that one needs to wait 27,000 events in order to ensure that the probability of observing a significant result is at least 0.95.  Business intelligence readers may be more interested in numbers such as p* = 0.0042 and p0 = 0.004 typical of so called click-through rates.  In this case, it would take more than 1 million events even though there is a true improvement of 5% in performance.</i> </p>

  <form id="myForm">
    <div style="display:inline-block;">
      <label for="pStar">p* = </label>
      <input type="text" name="pStar" placeholder="Choose a true success rate" size="21">
    </div>
    <div style="display:inline-block;">
      <label for="pBaseLine">p0 = </label>
      <input type="text" name="pBaseLine" placeholder="Choose a baseline rate" size="21">
    </div>
    <input type="submit" id="params_submit" style="background-color: #FFFFC2; border-color: #000000; padding: 4px 8px; border-radius: 4px; border: 1px solid;" value="Get required number of observations"/> <br> <br>
    <span class="error"></span>
  </form>

  <div><div id="plotConv" style="display:inline-block;width:49%;"></div><div id="plotHisto" style="display:inline-block;width:49%"></div></div>

  <div id="explanationContainer" style="width:80%;margin-left:10%;background-color: #FFFFC2; border-color: #000000; border-radius: 4px; border: 1px solid; padding: 4px 8px;">
  <p id="result"></p>
  <p id="graphExplanation">
    The red curve shows the evolution of the probability of observing a significant p-value as a function of the number of events.  The blue histogram shows the position of the baseline rate shown as a vertical green line compared to the probability density function of the empirical success rate p constructed from the underlying Binomial trials with true success rate p*.  Note that the standard deviation is selected by taking N = N*. </p>
  </div>

</div>
</div> 
</div>

<!-- <hr style="height:45px;border:none;color:#787878;background-color:#787878 ;" /> -->

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89531449-1', 'auto');
  ga('send', 'pageview');

</script>

<footer>
<p>More in-depth <a href="https://github.com/Ranlot/ab-testing">discussion</a> along with source code and mathematical derivations.</p>
</footer> 

</body>
</html>
